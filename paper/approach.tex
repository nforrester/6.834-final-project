\documentclass{article}

\usepackage[colorlinks=true]{hyperref}
\usepackage{amsmath}

\begin{document}
\section{Goal}
An algorithm is proposed to effectively play the game
\href{http://en.wikipedia.org/wiki/Wikipedia:Six_degrees_of_Wikipedia}{6 degrees
  of Wikipedia}, operating under similar constraints as a human. 6 degrees of
Wikipedia is a race to navigate as fast as possible between two random Wikipedia
articles only by clicking on the links in the body of an article. For example, a
game round might require navigation from Chewbacca to Tussock (grass). The
player must first click on a body link on the Chewbacca article, and after that
a link on the article that the first page linked to, and
then another link on the new article, and continuing in such a manner until the
player reaches the goal article. This algorithm should be able to competitively
race against a human player.

\section{Constraints}
The algorithm must be constrained to perform under similar circumstances to
those a human player must perform under. If this was not the case, the algorithm
would have an unfair advantage over a human player in a race. In this context,
unfair seems an arbitrary and nebulous concept. Unfair shall be taken to mean
two things:

\begin{itemize}
\item The algorithm must access articles at the same speed that a human can. As
  it is running, the algorithm must fetch each webpage it is examining from the
  Wikipedia servers. This is a significant limitation in speed, especially if
  the computer running the algorithm is connected to the internet over Wi-Fi.
  Because this page fetch is very expensive in comparison to other operations,
  it makes it important to carefully consider which link to click on.

\item The algorithm cannot used stored structures that would be completely
  impossible for a human to have. For instance, the fastest algorithm would use
  a pre-computed mapping of the shortest paths between all pairs on the Wikipedia
  article graph. The Wikipedia article graph is a directed graph, where each
  article is a node and each link is an edge from the node for the linking
  article to the node for the linked article. This is the kind of information
  that is not at all comparable to what a human could, and would make a race
  between a human and the algorithm quite boring. 
\end{itemize}

Additionally, the second meaning of unfair above aligns with computational
constraints on this kind of project. The all-pairs shortest-paths example
discussed elucidates this. There are about 4.5 million articles in the English
Wikipedia. This means storing the shortest paths for all pairs would require
storing $(4.5 \text{ million})^2 \approx 20 \text{ trillion}$ paths. Even
assuming that only 100 bytes would be needed for each path (which is an
underestimate), this would require over 1 petabyte of storage. This kind of
requirement is unrealistic for this project.

Although of course those computation requirements could be eased by
approximations, that sort of approach wouldn't be in line with the spirit of the
problem, which is to face the same kind of problem a human would.

\section{Approach}
\subsection{Category Graph}
When playing this game, humans use their internal conceptual hierarchy to inform
search. They click on links that they think will bring them to concepts closer
to those on the goal page. The conceptual hierarchy can be thought of as a
concept graph, or knowledge graph. In Wikipedia articles can belong to
categories, and categories can have subcategories, and further subcategories.
These categories can be used to build a category graph: each category is a node,
and an edge connects each category and subcategory. This category graph can act
as an analog to the human knowledge graph.

The algorithm can use the category graph to inform search. If the algorithm
knows the categories that the current article is in, and the categories that the
goal article is in, it can calculate which categories are on the shortest path
between the initial article and the goal article, and navigate to links it
thinks are in categories on that path.

\subsection{Article-Category Correspondence}
Once on the Wikipedia page for an article, the titles of the articles the links
on the page travel to are observable. The algorithm need to know the probability
that the article with title $t_i$ belongs to category $c_j$, $P(t_i \in c_j)$.
The information the algorithm observers to calculate this probability is the
list of words $[w_1, w_2, \dots, w_n]$ that makes up $t_i$.  So, it is the following
that must be computed:

\[P(t_i \in c_j) = P(t_i \in c_j | t_i = [w_1, w_2, \dots, w_n])\]

This expression can be transformed through the application of Bayes' Rule.

\[P(t_i \in c_j | t_i = [w_1, w_2, \dots, w_n]) = \frac{P(t_i = [w_1, w_2,
  \dots, w_n] | t_i \in c_j) P(t_i \in c_j)}{P(t_i = [w_1, w_2, \dots, w_n])}\]

The expression for $P(t_i = [w_1, w_2, \dots, w_n] | t_i \in c_j)$ can be
simplified. It is reasonable to assume that, conditioned on the category an
article title is in, the appearance of a word in a title is independent of
other words in the title. This assumption makes computation tractable.

\begin{multline*}
P(t_i = [w_1, w_2, \dots, w_n] | t_i \in c_j) \\
\begin{aligned}
& = P(w_1 \in t_i \cap w_2 \in t_i \cap \dots \cap w_n \in t_i | t_i \in c_j) \\
& \approx P(w_1 \in t_i | t_i \in c_j) P(w_2 \in t_i | t_i \in c_j) \cdots P(w_n
\in t_i | t_i \in c_j) \\
\end{aligned}
\end{multline*}

Although this conditional independence assumption is certainly not entirely
accurate, hopefully it is close enough to reality to not negatively affect the
results.

The value of $P(w_1 \in t_i | t_i \in c_j)$ is based off of training data.
During training, the correspondences between the words in article titles and
categories those articles belong to are calculated. This calculation is
completed by counting how often a word $w_l$ appears in article titles $t$ that
are in category $c_j$. 

\[P(w_l \in t | t \in c_j) \propto \displaystyle \sum_{t \in c_j} \sum_l 
    \delta (w_l \in t)\]
such that
\[\displaystyle \sum_l P(w_l \in t | t \in c_j) = 1\]

The $\delta$ function means:
\[\delta (w_l \in t) = \begin{cases}
  1 & \text{if } w_l \in t \\
  0 & \text{if } w_l \notin t \\
\end{cases} \]

$P(t_i \in c_j)$ is also based off of training data.  This value is simple to
calculate:

\[P(t_i \in c_j) = \frac{\text{\# of articles in } c_j}{\text{\# of articles}}\]

The algorithm now has computable expressions for everything needed in the
calculation of $P(t_i \in c_j)$. 

\subsection{Search Strategy}
The basic heuristic the algorithm uses to inform search is the shortest paths on
the category graph between the current article and the goal article. This should
naturally send the algorithm moving towards more general categories when it is
farther from the goal article, and towards more specific articles when it is
closer to the goal article. This is a good general strategy similar to what a
human does, so this tendency is reinforced by explicitly encoding a weighting to
move in this fashion. A centrality measure is used as an analog for the
generality of a category, likely the eigenvector centrality or PageRank
centrality, depending on whether the category graph is analyzed as a directed
graph or an undirected graph. Thus, the algorithm incorporates the shortest path
between the initial and goal articles and information on category generality in
order to choose the target categories for the next link.  The algorithm then
chooses the target categories based on the calculated probability of the
articles being in those categories.

\section{Scope Limitations}
Graph analysis on all of the data available on Wikipedia is overwhelming for the
computing hardware available for a class project. In order to make this more
feasible, some limitations were placed on the parts of Wikipedia to be used in
the analysis. Some options that are being explored are listed below. These
limitations could be implemented individually, or in some combination:

\begin{itemize}
\item Consider only the $x$ most viewed articles. Only include categories that
  these articles are in.

\item Consider only the top $x$ categories ranked by number of pages in the
  category. Only include pages in those categories.

\item Consider only the top $x$ categories ranked by number of subcategories in the
  category. Only include pages in those categories.

\item Consider only articles with more than $x$ words (articles with less than
  that can be considered stubs even if they haven't been labeled as such, and
  small articles probably haven't been edited as much, and as such aren't as
  important). Only include categories those pages are in.
\end{itemize}

\end{document}